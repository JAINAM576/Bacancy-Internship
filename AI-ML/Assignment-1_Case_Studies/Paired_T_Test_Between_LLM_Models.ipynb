{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCFkJUFmunPb"
      },
      "source": [
        "# Statistical Comparison of Llama-2-70B vs Flan-T5-XXL Performance\n",
        "\n",
        "This analysis compares task-level performance scores between two large language models:\n",
        "- **Llama-2-70B-chat-hf** (Meta)\n",
        "- **Flan-T5-XXL** (Google)\n",
        "\n",
        "The evaluation covers **46 different benchmark tasks** across multiple domains including:\n",
        "- Big Bench Hard (BBH) tasks (reasoning, logic, language understanding)\n",
        "- GPQA (Graduate-level science questions)\n",
        "- IFEval (Instruction following)\n",
        "- MATH (Mathematical problem solving)\n",
        "- MMLU Pro (Multitask language understanding)\n",
        "- MuSR (Multistep soft reasoning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gu3uiGqZvEJM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdzp2HetvIt5"
      },
      "source": [
        "## 1. Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "k2YbPNCXutia"
      },
      "outputs": [],
      "source": [
        "# Each value represents the normalized accuracy on a specific benchmark task\n",
        "llama = np.array([\n",
        "    0.302204, 0.724000, 0.540107, 0.212000, 0.304000, 0.472000,\n",
        "    0.004000, 0.552000, 0.200000, 0.152000, 0.336000, 0.168000,\n",
        "    0.420000, 0.076000, 0.150685, 0.096000, 0.188000, 0.140000,\n",
        "    0.483146, 0.628000, 0.276000, 0.208000, 0.152000, 0.332000,\n",
        "    0.488000, 0.264262, 0.247475, 0.269231, 0.265625,\n",
        "    0.443623, 0.547962, 0.534196, 0.631894,\n",
        "    0.029456, 0.048860, 0.000000, 0.000000, 0.017857, 0.025974,\n",
        "    0.067358, 0.014815,\n",
        "    0.243268, 0.367725, 0.516000, 0.250000, 0.340000\n",
        "])\n",
        "\n",
        "# Task-level scores for Flan-T5-XXL\n",
        "flan = np.array([\n",
        "    0.503906, 0.516000, 0.604278, 0.540000, 0.676000, 0.548000,\n",
        "    0.180000, 0.704000, 0.552000, 0.608000, 0.712000, 0.608000,\n",
        "    0.608000, 0.420000, 0.417808, 0.600000, 0.552000, 0.504000,\n",
        "    0.764045, 0.700000, 0.292000, 0.152000, 0.112000, 0.268000,\n",
        "    0.520000, 0.270134, 0.257576, 0.278388, 0.265625,\n",
        "    0.157116, 0.282974, 0.158965, 0.282974,\n",
        "    0.010574, 0.019544, 0.016260, 0.000000, 0.000000, 0.025974,\n",
        "    0.010363, 0.000000,\n",
        "    0.234292, 0.420635, 0.516000, 0.281250, 0.468000\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-io94MJP1sZ",
        "outputId": "e632faf0-fe69-44f1-8593-ffe6d39a749f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total tasks compared: 46\n"
          ]
        }
      ],
      "source": [
        "task_names = [\n",
        "    'BBH', 'BBH Boolean Expr', 'BBH Causal Judge', 'BBH Date Understand',\n",
        "    'BBH Disambiguation', 'BBH Formal Fallacies', 'BBH Geometric Shapes',\n",
        "    'BBH Hyperbaton', 'BBH Logic Deduct 5obj', 'BBH Logic Deduct 7obj',\n",
        "    'BBH Logic Deduct 3obj', 'BBH Movie Rec', 'BBH Navigate', 'BBH Object Count',\n",
        "    'BBH Penguins Table', 'BBH Colored Objects', 'BBH Ruin Names',\n",
        "    'BBH Translation Error', 'BBH Snarks', 'BBH Sports', 'BBH Temporal Seq',\n",
        "    'BBH Track Shuffle 5obj', 'BBH Track Shuffle 7obj', 'BBH Track Shuffle 3obj',\n",
        "    'BBH Web of Lies', 'GPQA', 'GPQA Diamond', 'GPQA Extended', 'GPQA Main',\n",
        "    'IFEval Prompt Strict', 'IFEval Inst Strict', 'IFEval Prompt Loose',\n",
        "    'IFEval Inst Loose', 'MATH Hard', 'MATH Algebra', 'MATH Counting/Prob',\n",
        "    'MATH Geometry', 'MATH Intermediate', 'MATH Number Theory', 'MATH Prealgebra',\n",
        "    'MATH Precalculus', 'MMLU Pro', 'MuSR', 'MuSR Murder Mysteries',\n",
        "    'MuSR Object Placement', 'MuSR Team Allocation'\n",
        "]\n",
        "\n",
        "print(f\"Total tasks compared: {len(llama)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RF1HmnBvRzM",
        "outputId": "f952c762-e0ba-4d4b-bf2c-a686a225791c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DESCRIPTIVE STATISTICS\n",
            "\n",
            "Llama-2-70B:\n",
            "  Mean accuracy: 0.2767331086956522 (27.67331086956522%)\n",
            "  Median accuracy: 0.257131\n",
            "  Std deviation: 0.19691738024756902\n",
            "  Min: 0.0, Max: 0.724\n",
            "  Range: 0.724\n",
            "\n",
            "Flan-T5-XXL:\n",
            "  Mean accuracy: 0.36127567391304344 (36.127567391304346%)\n",
            "  Median accuracy: 0.354904\n",
            "  Std deviation: 0.2347930886058384\n",
            "  Min: 0.0, Max: 0.764045\n",
            "  Range: 0.764045\n",
            "\n",
            "Raw difference (Flan - Llama):\n",
            "  Mean: 0.08454256521739123 (8.454256521739122 percentage points)\n"
          ]
        }
      ],
      "source": [
        "print(\"DESCRIPTIVE STATISTICS\")\n",
        "\n",
        "\n",
        "print(f\"\\nLlama-2-70B:\")\n",
        "print(f\"  Mean accuracy: {llama.mean()} ({llama.mean()*100}%)\")\n",
        "print(f\"  Median accuracy: {np.median(llama)}\")\n",
        "print(f\"  Std deviation: {llama.std(ddof=1)}\")\n",
        "print(f\"  Min: {llama.min()}, Max: {llama.max()}\")\n",
        "print(f\"  Range: {llama.max() - llama.min()}\")\n",
        "\n",
        "print(f\"\\nFlan-T5-XXL:\")\n",
        "print(f\"  Mean accuracy: {flan.mean()} ({flan.mean()*100}%)\")\n",
        "print(f\"  Median accuracy: {np.median(flan)}\")\n",
        "print(f\"  Std deviation: {flan.std(ddof=1)}\")\n",
        "print(f\"  Min: {flan.min()}, Max: {flan.max()}\")\n",
        "print(f\"  Range: {flan.max() - flan.min()}\")\n",
        "\n",
        "print(f\"\\nRaw difference (Flan - Llama):\")\n",
        "print(f\"  Mean: {flan.mean() - llama.mean()} ({(flan.mean() - llama.mean())*100} percentage points)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tKhqy4jwaX-"
      },
      "source": [
        "## 3. Statistical Hypothesis Testing\n",
        "\n",
        "We use a **paired t-test** because:\n",
        "- Same tasks are evaluated for both models (paired observations)\n",
        "- We want to test if there's a significant difference in mean performance\n",
        "\n",
        "**Hypotheses:**\n",
        "- H₀ (Null): No difference in mean performance between the models\n",
        "- H₁ (Alternative): There is a significant difference in mean performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rX-el_OtwnQL",
        "outputId": "1692a990-8f10-4ee0-f494-37fa18260f3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Statistical Comparison Results \n",
            "\n",
            "  - The calculated t-statistic is: -2.7125787869472053\n",
            "  - The p-value obtained is: 0.009424124700017885\n",
            "  - Our significance level (alpha) is set at: 0.05\n",
            "2713 Conclusion: With a p-value of 0.009424124700017885 (which is less than 0.05), we can confidently say that there IS a statistically significant difference in performance between the two models.\n",
            "  This means it's unlikely this difference happened by chance. Flan-T5-XXL tends to perform differently than Llama-2-70B on these tasks.\n"
          ]
        }
      ],
      "source": [
        "t_stat, p_value = stats.ttest_rel(llama, flan)\n",
        "\n",
        "print(\"\\n Statistical Comparison Results \")\n",
        "print(f\"\\n  - The calculated t-statistic is: {t_stat}\")\n",
        "print(f\"  - The p-value obtained is: {p_value}\")\n",
        "print(f\"  - Our significance level (alpha) is set at: 0.05\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(f\"2713 Conclusion: With a p-value of {p_value} (which is less than 0.05), we can confidently say that there IS a statistically significant difference in performance between the two models.\")\n",
        "    print(f\"  This means it's unlikely this difference happened by chance. Flan-T5-XXL tends to perform differently than Llama-2-70B on these tasks.\")\n",
        "else:\n",
        "    print(f\"2718 Conclusion: Our p-value of {p_value} is not lower than our significance level (0.05).\")\n",
        "    print(f\"  Therefore, we do NOT have enough evidence to claim a statistically significant difference in performance between Llama-2-70B and Flan-T5-XXL based on this test.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IyfLfyUvrMn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "medibot1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
